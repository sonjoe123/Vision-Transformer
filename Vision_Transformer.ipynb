{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0ZQZyF7H5KZ"
   },
   "source": [
    "# Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0FofcmdPuDZ6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn #layers, loss functions...\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim #optimization method\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader #convert dataset into patches\n",
    "from torchvision import datasets, transforms #to access datasets and transformation to apply\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt #visualizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrDl1kfrH5Kd"
   },
   "source": [
    "# Set up Seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed so that in CPU, GPU nothing is going to be fully random and model behave the same throughout training\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e6vecRlH5Ke"
   },
   "source": [
    "## Checking to make sure using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5070 Ti\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phant\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\cuda\\__init__.py:287: UserWarning: \n",
      "NVIDIA GeForce RTX 5070 Ti with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5070 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUXk64izH5Kg"
   },
   "source": [
    "# Set up Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the hyperparameters\n",
    "# All of these can be changed to experiment and tune the model\n",
    "\n",
    "BATCH_SIZE = 128       # Number of images to process at once (higher = faster but uses more memory). Common to use powers of 2\n",
    "EPOCHS = 10            # Number of full passes through the training dataset (entire dataset pass through = 1 epoch)\n",
    "LEARNING_RATE = 3e-4   # Controls how much the model updates weights each step. Smaller = slower but more stable learning\n",
    "\n",
    "PATCH_SIZE = 4         # Size of each image patch (4x4). Usually it would be 16x16 but since this image is pretty small we split it into 4x4\n",
    "\n",
    "NUM_CLASSES = 10       # Output dimension of classifier. CIFAR-10 has 10 categories\n",
    "\n",
    "IMAGE_SIZE = 32        # Width/Height of input image (CIFAR-10 = 32x32)\n",
    "CHANNELS = 3           # Number of color channels (RGB = 3)\n",
    "\n",
    "EMBED_DIM = 256        # Dimension of each patch embedding (token). Core size of all token vectors, control the dim throughout\n",
    "NUM_HEADS = 8          # Number of attention heads. Splits each token into 8 parts for multi-head self-attention\n",
    "DEPTH = 6              # Number of transformer encoder blocks (layers stacked in the model). Each layer is a full block that contain everything\n",
    "\n",
    "MLP_DIM = 512              # Hidden dimension inside the MLP block in each transformer layer (usually 2–4× EMBED_DIM)\n",
    "DROP_RATE = 0.1        # Dropout probability — randomly drops some activations to prevent overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTfXw-oCH5Kh"
   },
   "source": [
    "# Define Image Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1ErcNMG4H5Kh"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define Image Transformations\n",
    "transform = transforms.Compose([ # Container chains multiple image transformation together\n",
    "    transforms.ToTensor(), # Convert PIL images to Pytorch tensors so we can pass them to the model\n",
    "    transforms.Normalize((0.5),(0.5)) # Normalize the images. Shift the value to [-1.0, 1.0]\n",
    "    # this make the model more stable, faster to converge (is when it reach a decent good spot - not always the best )\n",
    "    # help introduce negative number which will help with the gradient, activation steps,...\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwjVHhlnH5Kh"
   },
   "source": [
    "# Getting datasets and view it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8gBlR3lDH5Kh"
   },
   "outputs": [],
   "source": [
    "# Getting a Dataset\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root = \"data\", # where the data save at\n",
    "                                 train = True, # get the train or the test data\n",
    "                                 download = True, # true to download\n",
    "                                 transform = transform) # apply what transformation and use the one we create up there\n",
    "\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root = \"data\",\n",
    "                                train = False,\n",
    "                                download = True,\n",
    "                                transform = transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mOS3F4SbH5Kh",
    "outputId": "95bedb34-bbb9-41a2-c97c-f153f2abfec8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=0.5, std=0.5)\n",
       "           )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-lUWa8IiH5Ki",
    "outputId": "df579f5c-ac07-4ce7-be4e-9657c900eb2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=0.5, std=0.5)\n",
       "           )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Vx1sIv-H5Ki"
   },
   "source": [
    "# Converting datasets to dataloader\n",
    "- Right now the data is in form of PyTorch Datasets\n",
    "- Dataloader will turn our data into batches (mini-batches)\n",
    "#### Why?\n",
    "- More computationally efficient, less work for the hardware since it might not be able to look (store in memory) at 50000 images in one hit. So we break it into 128 images at a time (batch_size = 128)\n",
    "- Give neural network more chances to update its gradients per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KugEaIAkH5Ki"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                          batch_size = BATCH_SIZE,\n",
    "                          shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                         batch_size = BATCH_SIZE,\n",
    "                         shuffle = False) # not shuffle with the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "R2tqrRihH5Ki",
    "outputId": "2e3a4240-1188-4683-9468-939e763b6d93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loader:  (<torch.utils.data.dataloader.DataLoader object at 0x000001EBD71097F0>, <torch.utils.data.dataloader.DataLoader object at 0x000001EBD7066350>)\n",
      "Length of train_loader: 391 batches of 128\n",
      "Length of test_loader: 79 batches of 128\n"
     ]
    }
   ],
   "source": [
    "# Check the dataloader\n",
    "print(f\"Data Loader:  { train_loader, test_loader}\")\n",
    "print(f\"Length of train_loader: {len(train_loader)} batches of {train_loader.batch_size}\")\n",
    "print(f\"Length of test_loader: {len(test_loader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbAoL3diH5Ki"
   },
   "source": [
    "# Building Vision Transformer Model from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "## Transforming the images into tokens (input embedding + positional embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RUbraI2uH5Ki"
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
    "        super().__init__() # Initialize the class so we can use all features from nn.Module (PyTorch base class)\n",
    "        self.patch_size = patch_size # Save the patch size (4x4 here). We will use this to split the image\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_channels,    # Number of input channels (CIFAR-10 = 3 for RGB)\n",
    "            out_channels=embed_dim,     # Number of output channels (the embedding dimension for each patch vector)\n",
    "            kernel_size=patch_size,     # Size of the patch (e.g., 4) → creates patches of 4x4\n",
    "            stride=patch_size           # Move the kernel by patch size. so here each patch is 4x4 and we are moving 4 pixel at the time so there will not be any overlapping patches\n",
    "        )\n",
    "        # Use a convolution to split the image into non-overlapping patches\n",
    "        # Each patch gets projected into a vector of size EMBED_DIM (this acts like linear projection for each patch)\n",
    "\n",
    "        num_patches = (img_size // patch_size) ** 2 # Calculate how many patches per image. CIFAR-10: 32x32 / 4x4 = 8x8 = 64 patches\n",
    "\n",
    "        # Create a special CLS token (learnable vector) that will represent the entire image at the end\n",
    "        # 1x1x256 — 1 batch, 1 token, 256 features\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "       \n",
    "        # Positional embedding: tell the model the position of each patch (including CLS token)\n",
    "        # Shape = (1, 65, 256) → 64 patches + 1 CLS = 65 total positions\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor): # x is a batch of images and in this case 128 and have the shape (128, 3, 32, 32) - 128 images, 3 channels, 32H, 32W\n",
    "        B = x.size(0) # this is taking out the first value in that order so 128\n",
    "\n",
    "        x = self.proj(x) # apply linear projection onto this batch of images transform the shape into (128, 256, 8, 8) - 128 images, 1x256 vector to represent each 4x4 patch (token size), 8 patches across W, 8 patches across H -> 64 patches per image\n",
    "\n",
    "        # Step 1: flatten(2) → merge last two dims (8x8) into one\n",
    "        # Shape goes from (B, 256, 8, 8) → (B, 256, 64)\n",
    "        # Now we have 64 patch tokens per image, but still in (feature, token) format        \n",
    "\n",
    "        # Step 2: transpose(1, 2) → swap channel and token dimension\n",
    "        # Shape becomes (B, 64, 256)\n",
    "        # This makes it: 64 tokens per image, each with a 256-dim embedding\n",
    "        # Final shape is (batch_size, num_patches, embed_dim)\n",
    "        # This is the required input shape for transformer encoders\n",
    "\n",
    "        x = x.flatten(2).transpose(1,2)\n",
    "\n",
    "        # Copy this CLS token and assign to each image. - 1 means keep that number the same so we are increasing the first one into B which is 128\n",
    "        # by doing expand you are not wasting any memory space because these are not real copies they still point to the same original cls token, just for views\n",
    "        # later on when add these cls token into the image tensor then it will allocate its new memory but for now you will not waste any\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "\n",
    "        # it take the shape of CLS token and x and concatenating them together along the position 1 in the dimension.\n",
    "        # cls token has shape (128,1,256) x has (128,64,256) so this will make x become (128,65,256)\n",
    "        # at this step the cls token vector is real and will allocate memory\n",
    "        # we have to do the expand is because .cat() require all tensors to have the same batch dimension\n",
    "        x = torch.cat((cls_token,x), dim = 1)\n",
    "\n",
    "\n",
    "        x = x + self.pos_embed # adding the position embedding value into x just normal math 1 to 1\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it through the transformer encoder \n",
    "## MLP block for self-refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module is base class for all neural network modules\n",
    "# Your model should subclass this class\n",
    "# This is the Feed Forward network step\n",
    "# At this step, we already have all the attentions from each tokens. This step helps each token to improve itself\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features, # input size\n",
    "                 hidden_features, # the hidden layer size\n",
    "                 drop_rate): # how much dropout to apply this is to prevent overfitting\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # Fully Connected 1 is where we increase the dimension of the vector fc1(x) = xW + b. x will be 1x256, W or Weight will be 256x512, b or bias will be 1x512 this will increase the size of the token to 512\n",
    "        # Weights and bias are automatically created when we call nn.Linear. Both are learnable and will be updated during training\n",
    "        self.fc1 =  nn.Linear(in_features= in_features, # in 256\n",
    "                           out_features= hidden_features) # out 512\n",
    "        \n",
    "        # Fully Connected 2 is where we take the large hidden layers and shrink it down to its original size\n",
    "        self.fc2 = nn.Linear(in_features= hidden_features, # in 512\n",
    "                             out_features= in_features) # out 256\n",
    "        \n",
    "        # This is called Regularization step- randomly sets some value to 0 to prevent the model from relying too much on specific value\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First it will put the token through fc1 then apply activation function onto it. we import torch.nn.functional as F and we are using activation function GELU for this (a softer version of ReLU)\n",
    "        # Then we apply regularization on to it\n",
    "        x = self.dropout(F.gelu(self.fc1(x))) \n",
    "\n",
    "        # Then we override x itself again putting it through fc2 and then apply regularization on it\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Transformer Encoder Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is 1 Transformer Encoder block. We will stack many of these later (like DEPTH = 6 means 6 of these blocks)\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim,   # The dimension of the token (like 256)\n",
    "                 num_heads,   # How many heads for multi-head attention\n",
    "                 mlp_dim,     # Hidden size inside the MLP block (e.g. 512)\n",
    "                 drop_rate):  # Dropout rate to avoid overfitting\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # First we apply Layer Normalization to stabilize training — this is like smoothing out the data\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Then we apply Multi-head Self Attention — this is the step where each token will attend to all other tokens in the image\n",
    "        # It takes 3 inputs: Query, Key, and Value — in ViT we use the same token for all 3\n",
    "        # dropout is also applied inside here\n",
    "        # batch_first = True because our input shape is (batch_size, num_tokens, embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate, batch_first=True)\n",
    "\n",
    "        # After attention, we normalize again before going into the MLP block\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # MLP block — this helps each token refine itself after the attention step\n",
    "        # It uses the GELU activation and dropout that we defined earlier\n",
    "        self.mlp = MLP(embed_dim, mlp_dim, drop_rate)\n",
    "\n",
    "\n",
    "    def forward (self, x):\n",
    "        # Step 1: we normalize all the input vectors. Attention requires Q, K, and V and it is all the same here which is norm1(x)\n",
    "        # Step 2: we apply self attention onto these 3 vectors and the output will be [attn_outputs, attn_weights] and we only want the outputs so [0].\n",
    "        # attn_weights to see how does each token attends to each other but only use this for visualization\n",
    "        # Step 3: then we add this attention output back to its original self (Residual Connection). This help to keep the original information and adds new learned info\n",
    "        x = x + self.attn(self.norm1(x),self.norm1(x), self.norm1(x))[0]\n",
    "\n",
    "        # Same here: we apply the normalization again before sending it to mlp then we use residual connection for this \n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim, drop_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # This handles the image patch splitting, linear projection, CLS token, and position embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "\n",
    "        # This builds a stack of TransformerEncoderLayer blocks (how many = depth)\n",
    "        # Each block includes: LayerNorm → Attention → MLP + residual\n",
    "        # [] This creates a list of TransformerEncoderLayer instances depends on how many depth\n",
    "        # the * helps to unpack them and feed them 1 by 1 like having a bag of candy instead of giving them all at once, give each candy at a time\n",
    "        # the result will be reused after each layer: layer 2 will use result from layer 1 and so on\n",
    "        # have to follow this syntax and put the for after the function. this is called list comprehension follow this form [expression for item in iterable]\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Final normalization layer after all encoder layers\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Final classification head (MLP): takes the CLS token and maps to 10 output classes\n",
    "        # Here it performs a linear transformation from 256 value-vector to a 10 value-vector since we have 10 classes. 1 image will be a vector of 10 with each value related to the 10 class\n",
    "        # Follow the same format as any other linear transformation so Weight and bias will also start out as random and will be updated later through backpropagation\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)       # Step 1: Convert image into tokens + CLS + positional embedding\n",
    "        x = self.encoder(x)           # Step 2: Pass through stacked Transformer blocks\n",
    "        x = self.norm(x)              # Step 3: Apply final LayerNorm after encoder stack\n",
    "        cls_token = x[:, 0]           # Step 4: Extract the CLS token. So x has shape (batch_size, num_tokens, embed_dim) by using : means all rows (all 128 images in a batch)\n",
    "                                        # 0 in the second position means take the first token and since we push CLS to the first position in the num_token, we wil get 128 CLS\n",
    "        return self.head(cls_token)   # Step 5: Pass CLS token into final classifier → output: (batch_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate model\n",
    "model = VisionTransformer(\n",
    "    IMAGE_SIZE, PATCH_SIZE, CHANNELS, NUM_CLASSES,\n",
    "    EMBED_DIM, DEPTH, NUM_HEADS, MLP_DIM, DROP_RATE\n",
    ").to(device)\n",
    "\n",
    "# Starting the model and send it to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbedding(\n",
       "    (proj): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerEncoderLayer(\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Measure how wrong our model is.\n",
    "# This is the loss function where it applies softmax to the logits (the output of the self.head()) so that it wil turn to probabilities that correlate with the classes (all will add up to 1)\n",
    "# Then it will compare to the true label. It will apply a negative log to the correspond position value in the vector. The lower the result the better. Output will be a single number\n",
    "\n",
    "\n",
    "# This is optimizer and we use \"Adam (a optimization method)\" to help update every single trainable parameters in the model\n",
    "# model.parameter grabs all the learnable parameters (weights and biases) in my model \n",
    "optimizer = torch.optim.Adam(params=model.parameters(), # update our model's parameters to try and reduce the loss\n",
    "                             lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop Setup \n",
    "## Main stuff here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    # Set the mode of the model into training\n",
    "    # PyTorch layers like dropout or layernorm are mode-aware. They only behave correctly if they are in the right mode\n",
    "    # You just need to activate this when we want to train the model make sure its the right mode\n",
    "    model.train()\n",
    "\n",
    "    # Loss tells you how wrong the model is. Each image has one loss, and the batch loss is the average of all image losses in that batch (e.g., 128 images).\n",
    "    # total_loss accumulates the scaled batch losses across all batches so we can compute the final average loss for the whole epoch.\n",
    "    # correct keeps track of how many predictions were correct (used to calculate accuracy later).\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    # CIFAR10 return data in the form (input, target) so in here x will be inputs and y will be labels\n",
    "    # Here it is looping through each batch with its correspond labels\n",
    "    for x, y in loader:\n",
    "        \n",
    "        # Moving (Sending) our data into the target device (GPU)\n",
    "        # Have to also move the data to the GPU otherwise the model is on GPU and data is on CPU, they can't communicate\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Resetting the gradient value each time we run a new batch since we don't to accumulate it through each time which will lead to wrong result\n",
    "        optimizer.zero_grad()\n",
    "        # 1. Forward pass (model outputs raw logits)\n",
    "        # This step is basically get the logits output of each image\n",
    "        out = model(x)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        # It applies softmax to the logits value then compare it to \"y\" which is the true label\n",
    "        loss = criterion(out, y)\n",
    "        \n",
    "        # 3. Perform backpropagation\n",
    "        # Traverse the entire computational graph. When we call model(x), it already put all the function that we create into like a same bag and PyTorch will build a computation graph behind it\n",
    "        # Every single steps that requires to have weights and bias will have their own W and b whenever we created them using PyTorch so when the system traverse,\n",
    "        #  the gradient value of each will be save in a .grad in that function behind the scene\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Perform Gradient Descent\n",
    "        # This is the step where PyTorch updates all the model's parameters (weights and bias).\n",
    "        # It looks at the .grad value for each parameter (calculated from loss.backward()).\n",
    "        # Then it adjusts the weights and biases slightly to reduce the loss — this is called Gradient Descent.\n",
    "        # In simple terms, it moves the weights in the direction that makes the model less wrong (closer to the correct answer).\n",
    "        # optimizer and criterion are linked to each other is because when we create optimizer we called model.parameter which make optimizer connects to all the parameter\n",
    "        #  and since we are saving .grad in the parameter it can access that gradient value\n",
    "        optimizer.step()\n",
    "\n",
    "        # the .item() helps to convert tensor value into a Python number even since loss is already a single number but still it was in tensor format\n",
    "        # the loss we calculate above are actually the average loss of the whole batch so now we are multiplying back with the batch size to get the actual loss value \n",
    "        total_loss += loss.item() * x.size(0)\n",
    "\n",
    "        # here we first use argmax(1) which mean look across dimension 1 since out has (128, 10) [128 rows, each with 10 softmax value from logits] so dimension 1 is the 10 part\n",
    "        # if the highest in the 10 value match with the position from the label (the highest softmax value guess correctly) we will sum it and turn it into a number using .item()\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "\n",
    "    # len(loader.dataset) is the total number of image in the entire dataset, not just this batch.\n",
    "    # This will return the average loss for each image for each epoch and overall accuracy\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
